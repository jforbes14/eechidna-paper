---
title: "modelling"
author: "Jeremy Forbes"
date: "18/03/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Modelling

### Pre-processing
With socio-demographic information now available for each electorate, each election is joined to the data  corresponding with its two-party preferred vote. Socio-demographic variables within each election year are standardized to have mean zero and variance one, to adjust for changing variable scales. For example, inflation-adjusted median rental prices increased across almost all electorates, with median rent of 200 dollars per week placing an electorate in the 90th percentile in 2001, but only the 30th percentile in 2016.

#### Dimension reduction
With only $N = 150$ observations (electorates) in each election and $p = 59$ socio-demographic variables in each cross-section, any model using all variables would face serious problems with multi-collinearity and over-fitting, likely leading to erroneous conclusions regarding variable significance. Therefore a form of dimension reduction is adopted that involves a *two-stage approach* that results in the identification of a reduced predictor set, for which models are fit. 

The first step[^2] is to combine socio-demographic variables that represent similar information into "factors" using principal component analysis (PCA). The scree plots of the principal components for each election all level off after four components, and the loadings of these four components are similar across the elections. Principal components are then computed on the combined set of socio-demographics across all six elections. A factor is created by combining several variables all have large loadings in a particular component and when there is an intuitive reason as to why these variables could represent common information. A loading with magnitude greater than 0.15 is considered large. After computing these sums, each factor is again standardized to have mean zero and variance one, within each election. 

Consider the `Incomes` factor as an illustration. Independent of principal components, we may suspect that median personal income, median household income and median family income are providing similar information about the financial wellbeing of an electorate. Their loadings in the first principal component are large (0.20, 0.21 and 0.22 respectively), which provides the evidence needed to combine these variables into a single factor, which is called `Incomes`.

This process reduces the predictor set to $p = 30$. However, it is still relatively large, and an additional variable selection method is required. Our target is to end up with a smaller subset of variables in the final stage when we model each election.

[^2]: A preliminary step involved removing all age bands, because age is represented by median age, and to remove some of the other 

### Model framework
An identical model specification is used across the six elections, with each election modelled separately. This allows for the socio-demographic effects to be estimated separately for each year, allowing for interpretation of temporal changes in these effects. This is preferable over a single longitudinal model because it avoids any concerns of undue bias stemming from an incorrectly imposed time-varying restriction on any variable. Without such restrictions, a pooled cross-sectional model does not yield any distinct advantage over separate models. The panel approach is avoided because of how frequently electoral boundaries change - electorates that have the same name across elections are not guaranteed to represent the same geographical region - meaning any fixed or random effects models would be difficult without implementing consistent boundaries requiring further imputation. 

For each cross-section, let the response variable be the two-party preferred vote in favour of the Liberal party, denoted $Y$, with $Y = 70$ representing a 70% preference for Liberal, 30% for Labor. Although $Y$ lies in the interval $(0,100)$, observed values are never very close to 0 or 100 (minimum $24.05 \%$ and maximum $74.90 \%$), so there is no need to impose the constraint of $Y \in [0,100]$. Furthermore, the response is found to be spatially correlated in each election (Moran's I test, $p \le 7\cdot10^{-15}$). This is expected, as electorates are aggregate spatial units, and hence the spatial structure of the data must modelled appropriately. 

The spatial error model [@Anselin99] is chosen because captures spatial heterogeneity by incorporating a spatially structured random effect vector [@LeSage2009]. In this context, the random effect thought of as capturing the political climate in each electorate, where the climate is correlated with the climate in neighbouring electorates. This functions under the assumption that the climate is independent of electoral socio-demographics, and that an electorate is equally correlated with any electorate that shares a part of its boundary. Spatial weights are calculated in accordance with these assumptions. The spatial error model is specified as follows:

Let $\rho$ be spatial autoregressive coefficient, $\boldsymbol v$ be a spherical error term, ${\bf W}$ be a matrix of spatial weights (containing information about the neighbouring regions), $\bf X$ be a matrix of socio-demographic covariates, $\boldsymbol \beta$ be a vector of regression coefficients and $\boldsymbol a$ be a spatially structured random effect vector.

$${\bf y} = {\bf X} {\boldsymbol \beta} + {\boldsymbol a}$$
and

$${\boldsymbol a} = \rho {\boldsymbol W} {\boldsymbol a} + {\boldsymbol v}$$
where 

$${\boldsymbol v} \sim N({\boldsymbol 0}, \sigma^2 {\boldsymbol I_n})$$.

so it can be written

$${\bf y} = {\bf X} {\boldsymbol \beta} + ({\bf I}_n-\rho {\bf W})^{-1}{\boldsymbol v}$$

Estimation is done using generalized least squares (using the `spdep` `R` package [@spdep]).

Table 4.1 details the resultant estimated model coefficients and their estimated standard errors for each of the six elections. These are interpreted in the next section.

*Model diagnostics are shown in Appendix \@ref(a:diagnostics), including the outcomes of tests for heteroskedasticity, residual non-normality, non-linear patterns in the residuals against included predictors, patterns in the residuals against missing predictors and influential points. These assessments did not indicate a major violation of any of the assumptions.*

----------------------------------------------------------------------------------------


## Modelling
- Address the issue of changing variable scales (rent as example)
- Dimension reduction via PCA to group variables together
- Choosing an appropriate model - discuss the spatial element
- Akaike weights to derive a variable set for models
- Final model specification


## Selecting a superset

The second stage of the data reduction process involves the selection of a superset of predictors that captures the five *most important* variables from each election. To determine the five most important variables for each election, Akaike weights are used to measure relative variable importance [@BurnhamAnderson2002]. This method is commonly used in ecological studies and is fundamentally based on the Akaike information criterion (AIC) [@Akaike73]. Akaike weights are produced using a method of scoring each model relative to a collection of models $M$ using AIC.

Let $\Delta_m = AIC_m - AIC_{min}$ denote the difference in AIC between models $m$ and that with minimum AIC in the model set $M$ for $m = 1,2,...,R$. The Akaike weights $w_m$ for model $m$ is calculated as

$$w_m = \frac {\exp(-\frac{1}{2}\Delta_m)} {\sum_{r=1} ^R \exp(-\frac{1}{2}\Delta_r)},$$
for $m = 1,...,R$.

Akaike weights represent the posterior probability that model $m$ is the best model in the set, without imposing any beliefs a priori, as $w_m \in (0,1)$ and $w_m$ sum to 1.

For each variable $j \in 1, ..., J$, the sum of the Akaike weights is computed over all models that include that variable, and denote this sum by $s_j$. That is, let

$$s_j = \sum_{m=1}^{R} w_m \cdot \text{I}(\text{variable } j \text{ used in model } m),$$
where $I(A)$ denotes the indicator function for A.

Variables are ordered according to $s_j$, which may be used as a measure of variable importance [@BurnhamAnderson2002]. The variable with largest $s_j$ is deemed to be the most important in the set for the purpose of modelling the response variable.

For each election, the model set $M$ used, for which Akaike weights are constructed, is the set of all possible five variable linear models. This involves fitting $R = {32 \choose 5} = 201,376$ models for each election, with five representing a compromise between an undue computational burden and capturing sufficient information. The five most important variables are selected from each election, and used to construct a variable superset obtained by taking the union of these six subsets. In doing so, a variable that appears to be important in a particular election (though it may be potentially unimportant in the others) is captured, along with those that are important in multiple elections. This superset contains the chosen predictors to be used in all of the final election models. Five variables are chosen from each election to ensure that this superset is not too large. The resultant superset contains twelve variables.

Before finalizing the common regression model to be fitted for each election year, two-way interactions are added using an iterative procedure. First, for each election, a model is fit using the superset of predictors as main effects only. Then all models with a single two-way interaction are computed, and the interaction tested using a likelihood-ratio test (1% significance level). The number of elections for which a given interaction is significant is tallied, and the interaction that is most frequently significant is selected to remain in the model. This process is repeated until no interaction is significant in more than one election.

Now that model specification has been determined, the final models are fit using the superset of 12 main effects and the four two-way interactions. The equation, estimated separately for each year, is as follows:

$\small{LNP_i = \beta_0 + \beta_1*\text{Extractive}_i + \beta_2*\text{Born_SE_Europe}_i + \beta_3*\text{ManagerAdminClericalSales}_i + \beta_4*\text{DeFacto}_i + \beta_5*\text{DiffAddress}_i + \beta_6*\text{Married}_i + \beta_7*\text{Born_UK}_i + \beta_8*\text{MedianAge}_i + \beta_9*\text{OneParent_House}_i + \beta_{10}*\text{OtherLanguageHome}_i + \beta_{11}*\text{Transformative}_i + \beta_{12}*\text{Unemployment}_i + \beta_{13}*\Big(\text{Extractive}_i*\text{OtherLanguageHome}_i\Big) + \beta_{14}*\Big(\text{DeFacto}_i*\text{OtherLanguageHome}_i\Big) + \beta_{15}*\Big(\text{Extractive}_i*\text{ManagerAdminClericalSales}_i\Big) + \beta_{16}*\Big(\text{MedianAge}_i*\text{OneParent_House}_i\Big) + \u_i}$

where ${\bf u} = ({\bf I_n} - \rho \cdot W) \cdot {\boldsymbol \varepsilon}$ and $\varepsilon_i \overset{iid}\sim N(0,\sigma^2)$.

Table *XXX* details the estimated model and their standard errors for each of the six elections. These are interpreted in the following section.


