---
title: "modelling"
author: "Jeremy Forbes"
date: "18/03/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Modelling
- Address the issue of changing variable scales (rent as example)
- Dimension reduction via PCA to group variables together
- Choosing an appropriate model - discuss the spatial element
- Akaike weights to derive a variable set for models
- Final model specification

## Modelling

With socio-demographic information now available for each electorate, each election is joined to the data  corresponding with its two-party preferred vote. Socio-demographic variables within each election year are standardized to have mean zero and variance one, to adjust for changing variable scales. For example, inflation-adjusted median rental prices increased across almost all electorates, with median rent of 200 dollars per week placing an electorate in the 90th percentile in 2001, but only the 30th percentile in 2016. The response, the two-party preferred vote in favour of the Liberal party (denoted $Y$), lies in the interval $(0,100)$, where $Y = 70$ would represent a 70% preference for Liberal, 30% for Labor. Since the observed values of $Y$ are never very close to 0 or 100 (minimum $24.05 \%$ and maximum $74.90 \%$), there is no need to impose the constraint of $Y \in [0,100]$.

An identical model specification is used across the six elections, with each election modelled separately. This allows for the socio-demographic effects to be estimated separately for each year, allowing for interpretation of temporal changes in these effects. This is preferable over a single longitudinal model because it avoids any concerns of undue bias stemming from an incorrectly imposed time-varying restriction on any variable. Without such restrictions, a pooled cross-sectional model does not yield any distinct advantage over separate models. The panel approach is avoided because of how frequently electoral boundaries change, so any fixed or random effects models would be inappropriate. 

For each election a spatial error model [@Anselin99] is to be estimated, as electorates have a spatial structure and it is reasonable to expect errors to have spatial dependence. This is a common approach for modelling aggregate spatial units [@Anselin2002]. Spatial weights are calculated by allowing each electorate to be equally correlated with any electorate that shares any part of its boundary. A Moran's I test of the two-party preferred vote finds overwhelming evidence significant spatial autocorrelation for each election ($p \le 7\cdot10^{-15}$). 

Let $\rho$ be the parameter governing the extent of spatial correlation, $\boldsymbol \varepsilon$ be a spherical error term, ${\bf W}$ be a matrix of spatial weights (containing information about the neighbouring regions), $\bf X$ is a matrix of socio-demographic covariates and $\boldsymbol \beta$ is a vector of regression coefficients.

The spatial error model is specified:

$${\bf y} = {\bf X} {\boldsymbol \beta} + ({\bf I}_n-\rho {\bf W})^{-1}{\boldsymbol \varepsilon}$$
Where $\varepsilon \sim N(0, \sigma^2)$. Estimation is done via maximum likelihood (using the `spdep` `R` package [@spdep]).

## Dimension reduction
With only $N = 150$ observations (electorates) in each election and $p = 59$ socio-demographic variables in each cross-section, any model using all variables would face serious problems with multi-collinearity and over-fitting, likely leading to erroneous conclusions regarding variable significance. Therefore a form of dimension reduction is adopted that involves a two-stage approach that results in the identification of a reduced predictor set, for which models are fit. 

The first step[^2] is to combine socio-demographic variables that represent similar information into "factors" using principal component analysis (PCA). The scree plots of the principal components for each election all level off after four components, and the loadings of these four components are similar across the elections. Principal components are then computed on the combined set of socio-demographics across all six elections. A factor is created by combining several variables all have large loadings in a particular component and when there is an intuitive reason as to why these variables could represent common information. A loading with magnitude greater than 0.15 is considered large. After computing these sums, each factor is again standardized to have mean zero and variance one, within each election. 

Consider the `Incomes` factor as an illustration. Independent of principal components, we may suspect that median personal income, median household income and median family income are providing similar information about the financial wellbeing of an electorate. Their loadings in the first principal component are large (0.20, 0.21 and 0.22 respectively), which provides the evidence needed to combine these variables into a single factor, which is called `Incomes`.

This process reduces the predictor set to $p = 30$. However, it is still relatively large, and an additional variable selection method is required. Our target is to end up with a smaller subset of variables in the final stage when we model each election.

[^2]: A preliminary step involved removing all age variables aside from median age.

## Selecting a superset

The second stage of the data reduction process involves the selection of a superset of predictors that captures the five *most important* variables from each election. To determine the five most important variables for each election, Akaike weights are used to measure relative variable importance [@BurnhamAnderson2002]. This method is commonly used in ecological studies and is fundamentally based on the Akaike information criterion (AIC) [@Akaike73]. Akaike weights are produced using a method of scoring each model relative to a collection of models $M$ using AIC.

Let $\Delta_m = AIC_m - AIC_{min}$ denote the difference in AIC between models $m$ and that with minimum AIC in the model set $M$ for $m = 1,2,...,R$. The Akaike weights $w_m$ for model $m$ is calculated as

$$w_m = \frac {\exp(-\frac{1}{2}\Delta_m)} {\sum_{r=1} ^R \exp(-\frac{1}{2}\Delta_r)},$$
for $m = 1,...,R$.

Akaike weights represent the posterior probability that model $m$ is the best model in the set, without imposing any beliefs a priori, as $w_m \in (0,1)$ and $w_m$ sum to 1.

For each variable $j \in 1, ..., J$, the sum of the Akaike weights is computed over all models that include that variable, and denote this sum by $s_j$. That is, let

$$s_j = \sum_{m=1}^{R} w_m \cdot \text{I}(\text{variable } j \text{ used in model } m),$$
where $I(A)$ denotes the indicator function for A.

Variables are ordered according to $s_j$, which may be used as a measure of variable importance [@BurnhamAnderson2002]. The variable with largest $s_j$ is deemed to be the most important in the set for the purpose of modelling the response variable.

For each election, the model set $M$ used, for which Akaike weights are constructed, is the set of all possible five variable linear models. This involves fitting $R = {32 \choose 5} = 201,376$ models for each election, with five representing a compromise between an undue computational burden and capturing sufficient information. The five most important variables are selected from each election, and used to construct a variable superset obtained by taking the union of these six subsets. In doing so, a variable that appears to be important in a particular election (though it may be potentially unimportant in the others) is captured, along with those that are important in multiple elections. This superset contains the chosen predictors to be used in all of the final election models. Five variables are chosen from each election to ensure that this superset is not too large. The resultant superset contains twelve variables.

Before finalizing the common regression model to be fitted for each election year, two-way interactions are added using an iterative procedure. First, for each election, a model is fit using the superset of predictors as main effects only. Then all models with a single two-way interaction are computed, and the interaction tested using a likelihood-ratio test (1% significance level). The number of elections for which a given interaction is significant is tallied, and the interaction that is most frequently significant is selected to remain in the model. This process is repeated until no interaction is significant in more than one election.

Now that model specification has been determined, the final models are fit using the superset of 12 main effects and the four two-way interactions. The equation, estimated separately for each year, is as follows:

$\small{LNP_i = \beta_0 + \beta_1*\text{Extractive}_i + \beta_2*\text{Born_SE_Europe}_i + \beta_3*\text{ManagerAdminClericalSales}_i + \beta_4*\text{DeFacto}_i + \beta_5*\text{DiffAddress}_i + \beta_6*\text{Married}_i + \beta_7*\text{Born_UK}_i + \beta_8*\text{MedianAge}_i + \beta_9*\text{OneParent_House}_i + \beta_{10}*\text{OtherLanguageHome}_i + \beta_{11}*\text{Transformative}_i + \beta_{12}*\text{Unemployment}_i + \beta_{13}*\Big(\text{Extractive}_i*\text{OtherLanguageHome}_i\Big) + \beta_{14}*\Big(\text{DeFacto}_i*\text{OtherLanguageHome}_i\Big) + \beta_{15}*\Big(\text{Extractive}_i*\text{ManagerAdminClericalSales}_i\Big) + \beta_{16}*\Big(\text{MedianAge}_i*\text{OneParent_House}_i\Big) + \u_i}$

where ${\bf u} = ({\bf I_n} - \rho \cdot W) \cdot {\boldsymbol \varepsilon}$ and $\varepsilon_i \overset{iid}\sim N(0,\sigma^2)$.

Table *XXX* details the estimated model and their standard errors for each of the six elections. These are interpreted in the following section.


